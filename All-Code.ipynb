{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Individual Data extraction and processing\n",
    "\n",
    "# Essential imports for multivariate modeling and Binance data handling\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from binance.client import Client\n",
    "from binance.exceptions import BinanceAPIException, BinanceRequestException\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Set your Binance API key and secret (leave blank for public endpoints)\n",
    "api_key = ''\n",
    "api_secret = ''\n",
    "\n",
    "# Create the client and verify the connection\n",
    "try:\n",
    "    client = Client(api_key, api_secret)\n",
    "    # Test connectivity (ping)\n",
    "    status = client.ping()\n",
    "    if status == {}:\n",
    "        print(\"Connection to Binance API successful!\")\n",
    "    else:\n",
    "        print(\"Unexpected ping response:\", status)\n",
    "except (BinanceAPIException, BinanceRequestException) as e:\n",
    "    print(f\"Binance API connection failed: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n",
    "\n",
    "# data timeframe\n",
    "start_str = \"2023-12-01 00:00:00\"\n",
    "end_str = \"2024-12-31 23:59:59\"\n",
    "start_dt = datetime.datetime.strptime(start_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "end_dt = datetime.datetime.strptime(end_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Pull all data in one go\n",
    "response = client.get_historical_klines(\n",
    "    \"BTCUSDT\",\n",
    "    Client.KLINE_INTERVAL_1MINUTE,\n",
    "    start_str,\n",
    "    end_str\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "def process_klines(response):\n",
    "    \"\"\"Convert raw kline data into a pandas DataFrame.\"\"\"\n",
    "    columns = [\n",
    "        \"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "        \"close_time\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "        \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"ignore\"\n",
    "    ]\n",
    "    df = pd.DataFrame(response, columns=columns)\n",
    "    df[\"open_time\"] = pd.to_datetime(df[\"open_time\"], unit='ms')\n",
    "    df.set_index(\"open_time\", inplace=True)\n",
    "    # Convert numeric columns\n",
    "    for col in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "        df[col] = df[col].astype(float)\n",
    "    df.sort_index(ascending=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def compute_rsi(series, period=14*24):\n",
    "    \"\"\"Compute the Relative Strength Index (RSI).\"\"\"\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0).ewm(alpha=1/period, adjust=False).mean()\n",
    "    loss = -delta.clip(upper=0).ewm(alpha=1/period, adjust=False).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_roc(series, period=24):\n",
    "    \"\"\"Compute the Rate of Change (ROC).\"\"\"\n",
    "    return series.pct_change(periods=period) * 100\n",
    "\n",
    "def compute_ema(series, period=12*24):\n",
    "    \"\"\"Compute the Exponential Moving Average (EMA).\"\"\"\n",
    "    return series.ewm(span=period, adjust=False).mean()\n",
    "\n",
    "# Process the raw data\n",
    "df = process_klines(response)\n",
    "\n",
    "# Calculate indicators\n",
    "df['RSI_14'] = compute_rsi(df['close'], period=14*24)\n",
    "df['ROC_1'] = compute_roc(df['close'], period=24)\n",
    "df['EMA_12'] = compute_ema(df['close'], period=12*24)\n",
    "\n",
    "# Select only the desired columns\n",
    "output_df = df[['close', 'volume', 'RSI_14', 'ROC_1', 'EMA_12']]\n",
    "\n",
    "# Save to CSV\n",
    "output_df.to_csv(\"sol_usdt_hourly_indicators.csv\")\n",
    "\n",
    "# Display the first few rows to the user\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"SOL/USDT Price & Indicators\", dataframe=output_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0752e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA based indexing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# List of DAO coin symbols\n",
    "dao_coins = ['btc', 'eth', 'sol', 'avax', 'bnb']  # Example DAO coins\n",
    "\n",
    "# Create data directory path\n",
    "data_dir = 'data'\n",
    "\n",
    "# Load and preprocess data\n",
    "dfs = []\n",
    "for symbol in dao_coins:\n",
    "    # Load each coin's data from the data directory\n",
    "    filename = os.path.join(data_dir, f\"{symbol}usdt_hourly_indicators.csv\")\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Convert open_time to datetime and set as index\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "    df.set_index('open_time', inplace=True)\n",
    "    \n",
    "    # Select only close price and volume\n",
    "    df = df[['close', 'volume']]\n",
    "    df.columns = [f'{symbol}_close', f'{symbol}_volume']\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "combined_df = pd.concat(dfs, axis=1)\n",
    "\n",
    "# Forward fill missing values (if any)\n",
    "combined_df.ffill(inplace=True)\n",
    "\n",
    "# Drop any remaining NA values\n",
    "combined_df.dropna(inplace=True)\n",
    "\n",
    "# Extract just close prices and volumes for PCA analysis\n",
    "close_prices = combined_df[[col for col in combined_df.columns if 'close' in col]]\n",
    "volume_data = combined_df[[col for col in combined_df.columns if 'volume' in col]]\n",
    "\n",
    "# Standardize the data\n",
    "X_close = preprocessing.StandardScaler().fit_transform(close_prices)\n",
    "X_volume = preprocessing.StandardScaler().fit_transform(volume_data)\n",
    "\n",
    "# Combine close prices and volumes (50/50 weighting)\n",
    "X_combined = np.hstack([X_close, X_volume])\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca_results = pca.fit_transform(X_combined)\n",
    "\n",
    "# Get sum of weights for first 3 principal components\n",
    "n_coins = len(dao_coins)\n",
    "close_weights = pca.components_[:, :n_coins]\n",
    "volume_weights = pca.components_[:, n_coins:]\n",
    "\n",
    "# Combine weights (equal weighting for price and volume)\n",
    "combined_weights = np.sum(close_weights + volume_weights, axis=0)\n",
    "\n",
    "# Create index fund weights (normalized)\n",
    "index_fund_weights = combined_weights / np.sum(np.abs(combined_weights))\n",
    "index_fund_tickers = dao_coins\n",
    "\n",
    "# New version (raw):\n",
    "raw_index_close = (close_prices * index_fund_weights).sum(axis=1)\n",
    "raw_index_volume = (volume_data * index_fund_weights).sum(axis=1)\n",
    "\n",
    "# Save both raw and normalized versions\n",
    "index_df = pd.DataFrame({\n",
    "    'close_raw': raw_index_close,\n",
    "    'volume_raw': raw_index_volume,\n",
    "    'close_norm': raw_index_close / raw_index_close.iloc[0],  # Normalized\n",
    "    'volume_norm': raw_index_volume / raw_index_volume.mean()  # Normalized\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "output_filename = 'lyrusdt_hourly_Indricators.csv'\n",
    "index_df.to_csv(output_filename)\n",
    "print(f\"Index data saved to {output_filename}\")\n",
    "\n",
    "# Visualization 1: Index Fund Weights\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(index_fund_tickers, index_fund_weights, color='skyblue', edgecolor='black')\n",
    "plt.title('DAO Index Fund Weights', fontsize=16)\n",
    "plt.xlabel('DAO Coin', fontsize=12)\n",
    "plt.ylabel('Weight in Index', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.savefig('DAO_Index_Weights.png')\n",
    "plt.show()\n",
    "\n",
    "# Create figure with 3 subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 14), sharex=True)\n",
    "\n",
    "# 1. Plot RAW PRICE comparison\n",
    "for coin in dao_coins:\n",
    "    ax1.plot(close_prices[f'{coin}_close'], \n",
    "             label=f'{coin}', alpha=0.5, linewidth=1)\n",
    "ax1.plot(raw_index_close, \n",
    "         label='DAO Index (Raw)', color='black', linewidth=2.5)\n",
    "ax1.set_title('Raw Price Comparison', fontsize=14)\n",
    "ax1.set_ylabel('Absolute Price', fontsize=12)\n",
    "ax1.legend(loc='upper left', fontsize=10)\n",
    "ax1.grid(linestyle='--', alpha=0.5)\n",
    "ax1.ticklabel_format(style='plain', axis='y')  # Disable scientific notation\n",
    "\n",
    "# 2. Plot NORMALIZED comparison\n",
    "for coin in dao_coins:\n",
    "    ax2.plot(close_prices[f'{coin}_close']/close_prices[f'{coin}_close'].iloc[0],\n",
    "             label=f'{coin}', alpha=0.5, linewidth=1)\n",
    "ax2.plot(raw_index_close/raw_index_close.iloc[0],\n",
    "         label='DAO Index (Normalized)', color='blue', linewidth=2.5, linestyle='--')\n",
    "ax2.set_title('Normalized Price Comparison (Base=100)', fontsize=14)\n",
    "ax2.set_ylabel('Normalized Price', fontsize=12)\n",
    "ax2.legend(loc='upper left', fontsize=10)\n",
    "ax2.grid(linestyle='--', alpha=0.5)\n",
    "\n",
    "# 3. Plot Volume comparison (normalized)\n",
    "for coin in dao_coins:\n",
    "    ax3.plot(volume_data[f'{coin}_volume']/volume_data[f'{coin}_volume'].mean(),\n",
    "             label=f'{coin}', alpha=0.3, linewidth=0.8)\n",
    "ax3.plot(raw_index_volume/raw_index_volume.mean(),\n",
    "         label='DAO Index Volume', color='red', linewidth=2)\n",
    "ax3.set_title('Normalized Volume Comparison', fontsize=14)\n",
    "ax3.set_xlabel('Date', fontsize=12)\n",
    "ax3.set_ylabel('Volume (Mean=1.0)', fontsize=12)\n",
    "ax3.legend(loc='upper left', fontsize=10)\n",
    "ax3.grid(linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('DAO_Index_Comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Correlation plot with ACTUAL values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(raw_index_close, raw_index_volume, \n",
    "            alpha=0.6, c='green', s=20, edgecolor='k')\n",
    "plt.title('Raw Index Price vs Volume Correlation', fontsize=16)\n",
    "plt.xlabel('Absolute Close Price', fontsize=12)\n",
    "plt.ylabel('Absolute Volume', fontsize=12)\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('DAO_Index_Correlation_Raw.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Bonus: Correlation plot with NORMALIZED values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(raw_index_close/raw_index_close.iloc[0], \n",
    "            raw_index_volume/raw_index_volume.mean(),\n",
    "            alpha=0.6, c='purple', s=20, edgecolor='k')\n",
    "plt.title('Normalized Index Price vs Volume Correlation', fontsize=16)\n",
    "plt.xlabel('Normalized Close Price', fontsize=12)\n",
    "plt.ylabel('Normalized Volume', fontsize=12)\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('DAO_Index_Correlation_Norm.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training -Model Evaluation - Data extraction - Data Processing\n",
    "\n",
    "# Essential imports for multivariate modeling and Binance data handling\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from binance.client import Client\n",
    "from binance.exceptions import BinanceAPIException, BinanceRequestException\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Set your Binance API key and secret (leave blank for public endpoints)\n",
    "api_key = ''\n",
    "api_secret = ''\n",
    "\n",
    "# Create the client and verify the connection\n",
    "try:\n",
    "    client = Client(api_key, api_secret)\n",
    "    # Test connectivity (ping)\n",
    "    status = client.ping()\n",
    "    if status == {}:\n",
    "        print(\"Connection to Binance API successful!\")\n",
    "    else:\n",
    "        print(\"Unexpected ping response:\", status)\n",
    "except (BinanceAPIException, BinanceRequestException) as e:\n",
    "    print(f\"Binance API connection failed: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    'WINDOW_SIZE': 12,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPOCHS': 50,\n",
    "    'LEARNING_RATE': 0.001,\n",
    "    'VAL_SPLIT': 0.1,\n",
    "    'TRAIN_SPLIT': 0.8,\n",
    "    'PATIENCE': 15,\n",
    "    'MAX_DATE': \"2023-12-31 23:59:59\",\n",
    "    'DATA_FILE_PATTERN': \"*_hourly_indicators.csv\",\n",
    "    'MODEL_FILE_PATTERN': \"model_{symbol}.keras\",\n",
    "    'LOG_LEVEL': logging.INFO\n",
    "}\n",
    "\n",
    "logging.basicConfig(level=CONFIG['LOG_LEVEL'], format='[%(levelname)s] %(message)s')\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def list_csv_files(pattern=None):\n",
    "    directory = \"Data\"\n",
    "    pattern = pattern or CONFIG['DATA_FILE_PATTERN']\n",
    "    return glob.glob(os.path.join(directory, pattern))\n",
    "\n",
    "def select_from_list(options, prompt_msg):\n",
    "    print(\"\\n\" + prompt_msg)\n",
    "    for i, option in enumerate(options):\n",
    "        print(f\"{i}. {option}\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(f\"Enter your choice (0-{len(options)-1}): \").strip()\n",
    "        try:\n",
    "            choice = int(user_input)\n",
    "            if 0 <= choice < len(options):\n",
    "                return options[choice]\n",
    "            print(f\"Please enter a number between 0 and {len(options)-1}\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "\n",
    "def text_input_widget(prompt):\n",
    "    return input(prompt + \": \").strip()\n",
    "\n",
    "def yes_no_widget(prompt):\n",
    "    while True:\n",
    "        user_input = input(prompt + \" (y/n): \").strip().lower()\n",
    "        if user_input in ('y', 'n'):\n",
    "            return user_input\n",
    "        print(\"Please enter 'y' or 'n'.\")\n",
    "\n",
    "def save_plot(fig, filename, symbol=None):\n",
    "    if symbol:\n",
    "        base, ext = os.path.splitext(filename)\n",
    "        filename = f\"{symbol}_{base}{ext}\"\n",
    "    fig.savefig(filename)\n",
    "    logging.info(f\"Plot saved to {filename}\")\n",
    "\n",
    "def save_metrics(metrics_df, filename, symbol=None):\n",
    "    if symbol:\n",
    "        base, ext = os.path.splitext(filename)\n",
    "        filename = f\"{symbol}_{base}{ext}\"\n",
    "    metrics_df.to_csv(filename, index=False)\n",
    "    logging.info(f\"Metrics saved to {filename}\")\n",
    "\n",
    "# --- Data Loading and Feature Engineering ---\n",
    "def process_klines(response):\n",
    "    columns = [\n",
    "        \"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\",\n",
    "        \"close_time\", \"quote_asset_volume\", \"number_of_trades\",\n",
    "        \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"ignore\"\n",
    "    ]\n",
    "    df = pd.DataFrame(response, columns=columns)\n",
    "    df[\"open_time\"] = pd.to_datetime(df[\"open_time\"], unit='ms')\n",
    "    df.set_index(\"open_time\", inplace=True)\n",
    "    df = df[[\"close\", \"volume\"]].astype(float)\n",
    "    df = df.sort_index(ascending=True)\n",
    "    return df\n",
    "\n",
    "def compute_rsi(series, period=14*24):\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0).ewm(alpha=1/period, adjust=False).mean()\n",
    "    loss = -delta.clip(upper=0).ewm(alpha=1/period, adjust=False).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_roc(series, period=24):\n",
    "    return series.pct_change(periods=period) * 100\n",
    "\n",
    "def compute_ema(series, period=12*24):\n",
    "    return series.ewm(span=period, adjust=False).mean()\n",
    "\n",
    "def add_indicators(df):\n",
    "    df['RSI_14'] = compute_rsi(df['close'], period=14*24)\n",
    "    df['ROC_1'] = compute_roc(df['close'], period=24)\n",
    "    df['EMA_12'] = compute_ema(df['close'], period=12*24)\n",
    "    return df\n",
    "\n",
    "def load_or_download_data(client=None):\n",
    "    csv_files = list_csv_files()\n",
    "    options = csv_files + [\"Pull new data\"]\n",
    "    selected_option = select_from_list(options, \"Select data file or pull new data:\")\n",
    "    \n",
    "    if selected_option != \"Pull new data\":\n",
    "        logging.info(f\"Loading data from: {selected_option}\")\n",
    "        # Extract first 3 characters from filename (without extension)\n",
    "        filename_base = os.path.splitext(os.path.basename(selected_option))[0]\n",
    "        symbol_or_prefix = filename_base[:6]  # First 3 characters\n",
    "        \n",
    "        # Load only the basic columns (close and volume)\n",
    "        df = pd.read_csv(selected_option, parse_dates=[\"open_time\"], index_col=\"open_time\")\n",
    "        \n",
    "        # Ensure we have the required basic columns\n",
    "        if 'close' not in df.columns or 'volume' not in df.columns:\n",
    "            raise ValueError(\"Loaded CSV must contain at least 'close' and 'volume' columns\")\n",
    "            \n",
    "        # Keep only close and volume (in case the file has other columns)\n",
    "        df = df[['close', 'volume']].astype(float)\n",
    "        \n",
    "        # Calculate indicators\n",
    "        df = add_indicators(df)\n",
    "        \n",
    "        # Save back to the same file (overwrite)\n",
    "        df.to_csv(selected_option)\n",
    "        logging.info(f\"Indicators calculated and data saved back to: {selected_option}\")\n",
    "        return df, symbol_or_prefix\n",
    "    \n",
    "    # Pull new data\n",
    "    symbol = text_input_widget(\"Enter the symbol of the crypto pair (e.g., BTCUSDT)\").upper()\n",
    "    if client is None or not symbol:\n",
    "        raise ValueError(\"Binance client and symbol must be provided for download.\")\n",
    "    \n",
    "    try:\n",
    "        client.get_symbol_info(symbol)\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Symbol '{symbol}' isn't recognized or not available on Binance.\")\n",
    "    \n",
    "    start_str = \"2017-01-01 00:00:00\"\n",
    "    end_str = \"2025-01-01 00:00:00\"\n",
    "    logging.info(\"Downloading data (this may take a while)...\")\n",
    "    response = client.get_historical_klines(\n",
    "        symbol,\n",
    "        client.KLINE_INTERVAL_1HOUR,\n",
    "        start_str,\n",
    "        end_str\n",
    "    )\n",
    "    logging.info(\"Data pulled.\")\n",
    "    df = process_klines(response)\n",
    "    df = add_indicators(df)\n",
    "    filename = f\"{symbol.lower()}_hourly_indicators.csv\"\n",
    "    df.to_csv(filename)\n",
    "    logging.info(f\"Data saved to {filename}\")\n",
    "    return df, symbol\n",
    "\n",
    "# --- Scaling Utilities ---\n",
    "class FeatureScaler:\n",
    "    def __init__(self, minmax_cols, std_cols):\n",
    "        self.minmax_cols = minmax_cols\n",
    "        self.std_cols = std_cols\n",
    "        self.mm_scaler = MinMaxScaler()\n",
    "        self.std_scaler = StandardScaler()\n",
    "    \n",
    "    def fit(self, df):\n",
    "        self.mm_scaler.fit(df[self.minmax_cols])\n",
    "        self.std_scaler.fit(df[self.std_cols])\n",
    "    \n",
    "    def transform(self, df):\n",
    "        scaled = df.copy()\n",
    "        scaled[self.minmax_cols] = self.mm_scaler.transform(df[self.minmax_cols])\n",
    "        scaled[self.std_cols] = self.std_scaler.transform(df[self.std_cols])\n",
    "        return scaled\n",
    "    \n",
    "    def inverse_transform_close(self, arr):\n",
    "        zeros = np.zeros((arr.shape[0], len(self.minmax_cols)-1))\n",
    "        arr_full = np.concatenate([arr.reshape(-1, 1), zeros], axis=1)\n",
    "        return self.mm_scaler.inverse_transform(arr_full)[:, 0]\n",
    "\n",
    "# --- Data Preparation ---\n",
    "def split_and_scale(features, scaler, config):\n",
    "    max_date = pd.Timestamp(config['MAX_DATE'])\n",
    "    features = features.loc[:max_date]\n",
    "    n_cut = len(features) - config['WINDOW_SIZE'] + 1\n",
    "    train_end = int(n_cut * config['TRAIN_SPLIT'])\n",
    "    val_end = train_end + int(n_cut * config['VAL_SPLIT'])\n",
    "    scaler.fit(features.iloc[:train_end])\n",
    "    train_data = scaler.transform(features.iloc[:train_end])\n",
    "    val_data = scaler.transform(features.iloc[train_end:val_end])\n",
    "    test_data = scaler.transform(features.iloc[val_end:n_cut])\n",
    "    return train_data, val_data, test_data, train_end, val_end, n_cut\n",
    "\n",
    "def df_to_x_y_multivariate(features, window_size):\n",
    "    features_numpy = features.to_numpy()\n",
    "    x, y = [], []\n",
    "    for i in range(len(features) - window_size):\n",
    "        x.append(features_numpy[i:i + window_size])\n",
    "        y.append(features_numpy[i + window_size][0])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "# --- Model Definition ---\n",
    "def build_gru_model(input_shape, lr):\n",
    "    model = Sequential([\n",
    "        GRU(64, return_sequences=True, input_shape=input_shape),\n",
    "        GRU(32),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=MeanSquaredError(),\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- Metrics and Plotting ---\n",
    "def compute_metrics(results, name):\n",
    "    rmse = np.sqrt(mean_squared_error(results['actuals'], results['predictions']))\n",
    "    mae = mean_absolute_error(results['actuals'], results['predictions'])\n",
    "    rmse_inv = np.sqrt(mean_squared_error(results['actuals_inv'], results['predictions_inv']))\n",
    "    mae_inv = mean_absolute_error(results['actuals_inv'], results['predictions_inv'])\n",
    "    mape_inv = mean_absolute_percentage_error(results['actuals_inv'], results['predictions_inv']) * 100\n",
    "    return [\n",
    "        [f'{name} (scaled)', rmse, mae, \"N/A\"],\n",
    "        [f'{name} (inversed)', rmse_inv, mae_inv, mape_inv]\n",
    "    ]\n",
    "\n",
    "def plot_results(ax, idx, preds, actuals, title, ylabel=None):\n",
    "    ax.plot(idx[:1000], preds[:1000], label='Predictions', color='orange')\n",
    "    ax.plot(idx[:1000], actuals[:1000], label='Actuals', color='blue')\n",
    "    ax.set_title(title)\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "\n",
    "def plot_all_results(train_results, val_results, test_results, config):\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "    plot_results(axes[0, 0], range(1000), train_results['predictions'], train_results['actuals'], 'Train Predictions vs Actuals (Scaled)')\n",
    "    plot_results(axes[0, 1], train_results['index'], train_results['predictions_inv'], train_results['actuals_inv'], 'Train Predictions vs Actuals (Inversed)', ylabel='Close Price')\n",
    "    plot_results(axes[1, 0], range(1000), val_results['predictions'], val_results['actuals'], 'Validation Predictions vs Actuals (Scaled)')\n",
    "    plot_results(axes[1, 1], val_results['index'], val_results['predictions_inv'], val_results['actuals_inv'], 'Validation Predictions vs Actuals (Inversed)', ylabel='Close Price')\n",
    "    plot_results(axes[2, 0], range(1000), test_results['predictions'], test_results['actuals'], 'Test Predictions vs Actuals (Scaled)')\n",
    "    plot_results(axes[2, 1], test_results['index'], test_results['predictions_inv'], test_results['actuals_inv'], 'Test Predictions vs Actuals (Inversed)', ylabel='Close Price')\n",
    "    plt.tight_layout()\n",
    "    save_plot(fig, \"predictions_vs_actuals.png\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_loss(history):\n",
    "    fig_loss = plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('GRU Model Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    save_plot(fig_loss, \"training_loss_curve.png\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Evaluation Logic ---\n",
    "def get_results(model, x, y, scaler, data_idx):\n",
    "    preds = model.predict(x, verbose=0).flatten()\n",
    "    preds_inv = scaler.inverse_transform_close(preds)\n",
    "    actuals_inv = scaler.inverse_transform_close(y)\n",
    "    idx = data_idx.values if hasattr(data_idx, 'values') else data_idx\n",
    "    return {\n",
    "        'predictions': preds,\n",
    "        'actuals': y,\n",
    "        'predictions_inv': preds_inv,\n",
    "        'actuals_inv': actuals_inv,\n",
    "        'index': idx\n",
    "    }\n",
    "\n",
    "def evaluate_and_plot(model, x_train, y_train, x_val, y_val, x_test, y_test, scaler, train_idx, val_idx, test_idx, config):\n",
    "    train_results = get_results(model, x_train, y_train, scaler, train_idx)\n",
    "    val_results = get_results(model, x_val, y_val, scaler, val_idx)\n",
    "    test_results = get_results(model, x_test, y_test, scaler, test_idx)\n",
    "\n",
    "    metrics = []\n",
    "    for split_name, results in zip(['Train', 'Validation', 'Test'], [train_results, val_results, test_results]):\n",
    "        metrics.extend(compute_metrics(results, split_name))\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics, columns=['Split', 'RMSE', 'MAE', 'MAPE (%)'])\n",
    "    print(metrics_df.to_string(index=False))\n",
    "    save_metrics(metrics_df, \"evaluation_metrics.csv\")\n",
    "\n",
    "    train_rmse = metrics_df.loc[metrics_df['Split'] == 'Train (inversed)', 'RMSE'].values[0]\n",
    "    val_rmse = metrics_df.loc[metrics_df['Split'] == 'Validation (inversed)', 'RMSE'].values[0]\n",
    "    test_rmse = metrics_df.loc[metrics_df['Split'] == 'Test (inversed)', 'RMSE'].values[0]\n",
    "    ratio = val_rmse / train_rmse if train_rmse > 0 else np.inf\n",
    "\n",
    "    if ratio > 1.5 and val_rmse > 1.5 * test_rmse:\n",
    "        raise Exception(\"Model is likely OVERFITTING: Validation error is much higher than training error.\")\n",
    "    elif train_rmse > 2 * test_rmse and val_rmse > 2 * test_rmse:\n",
    "        raise Exception(\"Model is likely UNDERFITTING: Both training and validation errors are high.\")\n",
    "    else:\n",
    "        logging.info(\"Model fit is acceptable. Proceeding to plot results...\")\n",
    "\n",
    "    plot_all_results(train_results, val_results, test_results, config)\n",
    "\n",
    "# --- Main Workflow ---\n",
    "def main(client=None):\n",
    "    # 1. Data Loading and Feature Engineering\n",
    "    df, data_file = load_or_download_data(client=client)\n",
    "    print(\"Data loaded and indicators added. Proceeding to feature selection...\")\n",
    "\n",
    "    features = df[['close', 'volume', 'RSI_14', 'ROC_1', 'EMA_12']].dropna()\n",
    "    print(\"Features selected. Proceeding to scaling and splitting...\")\n",
    "\n",
    "    # 2. Scaling and Splitting\n",
    "    minmax_cols = ['close', 'volume', 'RSI_14', 'EMA_12']\n",
    "    std_cols = ['ROC_1']\n",
    "    scaler = FeatureScaler(minmax_cols, std_cols)\n",
    "    train_data, val_data, test_data, train_end, val_end, n_cut = split_and_scale(features, scaler, CONFIG)\n",
    "    print(\"Data scaled and split. Proceeding to LSTM/GRU input preparation...\")\n",
    "\n",
    "    # 3. Prepare LSTM/GRU Inputs\n",
    "    x_train, y_train = df_to_x_y_multivariate(train_data, CONFIG['WINDOW_SIZE'])\n",
    "    x_val, y_val = df_to_x_y_multivariate(val_data, CONFIG['WINDOW_SIZE'])\n",
    "    x_test, y_test = df_to_x_y_multivariate(test_data, CONFIG['WINDOW_SIZE'])\n",
    "    print(\"First 5 rows of train_data:\")\n",
    "    print(pd.DataFrame(train_data, columns=features.columns).head())\n",
    "    print(\"\\nFirst 5 rows of val_data:\")\n",
    "    print(pd.DataFrame(val_data, columns=features.columns).head())\n",
    "    print(\"\\nFirst 5 rows of test_data:\")\n",
    "    print(pd.DataFrame(test_data, columns=features.columns).head())\n",
    "    print(\"LSTM/GRU inputs prepared. Proceeding to model definition...\")\n",
    "    print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "    # 4. Model Definition\n",
    "    input_shape = (CONFIG['WINDOW_SIZE'], train_data.shape[1])\n",
    "    symbol = data_file.split('_')[0] if '_' in data_file else 'model'\n",
    "    model_filename = f\"model_({data_file if isinstance(data_file, str) else data_file}).keras\"\n",
    "    model = build_gru_model(input_shape, CONFIG['LEARNING_RATE'])\n",
    "    model.summary()\n",
    "    print(\"Model defined. Waiting for user confirmation to start training...\")\n",
    "\n",
    "    # 5. Training\n",
    "    if yes_no_widget(\"Continue with model generation?\") != \"y\":\n",
    "        raise RuntimeError(\"Model generation aborted by user.\")\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(model_filename, save_best_only=True),\n",
    "        EarlyStopping(monitor='val_loss', patience=CONFIG['PATIENCE'], restore_best_weights=True)\n",
    "    ]\n",
    "\n",
    "    logging.info(\"Starting model training...\")\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=CONFIG['EPOCHS'],\n",
    "        batch_size=CONFIG['BATCH_SIZE'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=2,\n",
    "        shuffle=False\n",
    "    )\n",
    "    plot_training_loss(history)\n",
    "    print(\"Model training complete. Waiting for user confirmation to start evaluation...\")\n",
    "\n",
    "    # 6. Evaluation\n",
    "    if yes_no_widget(\"Continue with model evaluation and plotting?\") != \"y\":\n",
    "        raise RuntimeError(\"Model evaluation aborted by user.\")\n",
    "\n",
    "    model_files = [f for f in os.listdir('.') if f.endswith('.h5') or f.endswith('.keras')]\n",
    "    if not model_files:\n",
    "        raise FileNotFoundError(\"No saved model (.h5 or .keras) found in the directory.\")\n",
    "    latest_model_file = max(model_files, key=lambda f: os.path.getmtime(os.path.join('.', f)))\n",
    "    logging.info(f\"Loading latest model: {latest_model_file}\")\n",
    "    model = tf.keras.models.load_model(latest_model_file)\n",
    "\n",
    "    train_idx = features.iloc[CONFIG['WINDOW_SIZE']:train_end].index\n",
    "    val_idx = features.iloc[train_end+CONFIG['WINDOW_SIZE']:val_end].index\n",
    "    test_idx = features.iloc[val_end+CONFIG['WINDOW_SIZE']:n_cut].index\n",
    "\n",
    "    evaluate_and_plot(\n",
    "        model, x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "        scaler, train_idx, val_idx, test_idx, CONFIG\n",
    "    )\n",
    "    print(\"All steps completed successfully. Model evaluation and plotting finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Constants\n",
    "MINMAX_COLS = ['close', 'volume', 'RSI_14', 'EMA_12']\n",
    "STD_COLS = ['ROC_1']\n",
    "\n",
    "class FeatureScaler:\n",
    "    \"\"\"Handles feature scaling with different methods for different columns\"\"\"\n",
    "    def __init__(self, minmax_cols, std_cols):\n",
    "        self.minmax_cols = minmax_cols\n",
    "        self.std_cols = std_cols\n",
    "        self.mm_scaler = MinMaxScaler()\n",
    "        self.std_scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, df):\n",
    "        self.mm_scaler.fit(df[self.minmax_cols])\n",
    "        self.std_scaler.fit(df[self.std_cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        scaled = df.copy()\n",
    "        scaled[self.minmax_cols] = self.mm_scaler.transform(df[self.minmax_cols])\n",
    "        scaled[self.std_cols] = self.std_scaler.transform(df[self.std_cols])\n",
    "        return scaled\n",
    "\n",
    "    def inverse_transform_close(self, arr):\n",
    "        dummy = np.zeros((len(arr), len(self.minmax_cols)))\n",
    "        dummy[:, 0] = arr\n",
    "        return self.mm_scaler.inverse_transform(dummy)[:, 0]\n",
    "\n",
    "\n",
    "def df_to_sequences(df, window_size):\n",
    "    \"\"\"Convert DataFrame to sequences for LSTM/GRU training and forecasting\"\"\"\n",
    "    values = df.values\n",
    "    X, y, current_prices = [], [], []\n",
    "    for i in range(len(values) - window_size):\n",
    "        X.append(values[i : i + window_size])\n",
    "        y.append(values[i + window_size][0])\n",
    "        current_prices.append(values[i + window_size - 1][0])\n",
    "    return np.array(X), np.array(y), np.array(current_prices)\n",
    "\n",
    "\n",
    "def prepare_quarter_data(df, window_size, scaler=None):\n",
    "    \"\"\"Prepare data for a quarter with optional pre-fitted scaler\"\"\"\n",
    "    features = df[['close', 'volume', 'ROC_1', 'RSI_14', 'EMA_12']].dropna()\n",
    "    if scaler is None:\n",
    "        scaler = FeatureScaler(minmax_cols=MINMAX_COLS, std_cols=STD_COLS).fit(features)\n",
    "    scaled = scaler.transform(features)\n",
    "    X, y, current = df_to_sequences(scaled, window_size)\n",
    "    return X, y, current, scaler\n",
    "\n",
    "\n",
    "def forecast_quarterly(\n",
    "    ASSETS, DATA_DIR, MODEL_DIR, OUTPUT_DIR,\n",
    "    window_size=12,\n",
    "    date_start='2023-12-01',\n",
    "    date_end='2025-01-01',\n",
    "    train_epochs_q0=5,\n",
    "    train_epochs=5,\n",
    "    learning_rate=1e-3\n",
    "):\n",
    "    # 1. Load raw data\n",
    "    raw_data = {\n",
    "        asset: pd.read_csv(\n",
    "            os.path.join(DATA_DIR, f\"{asset}usdt_hourly_indicators.csv\"),\n",
    "            parse_dates=['open_time']\n",
    "        )\n",
    "        for asset in ASSETS\n",
    "    }\n",
    "\n",
    "    # Define Q0: initial December 2023 period\n",
    "    q0_start = pd.Timestamp('2023-12-01')\n",
    "    q0_end = pd.Timestamp('2023-12-31')\n",
    "    # Define quarters Q1-Q4 of 2024\n",
    "    quarters = pd.period_range('2024-01-01', '2024-12-31', freq='Q')\n",
    "\n",
    "    for asset, df in raw_data.items():\n",
    "        # 2. Filter overall date range\n",
    "        df = df.set_index('open_time')\n",
    "        df = df.loc[date_start:date_end].reset_index()\n",
    "\n",
    "        # Load pre-trained model\n",
    "        model_path = os.path.join(MODEL_DIR, f\"model_{asset}usdt.keras\")\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "        all_results = []\n",
    "        prev_scaler = None\n",
    "\n",
    "        # === Handle Q0 (Dec 2023) ===\n",
    "        df_q0 = df[(df['open_time'] >= q0_start) & (df['open_time'] <= q0_end)].copy()\n",
    "        if not df_q0.empty:\n",
    "            X0, y0, current0, prev_scaler = prepare_quarter_data(df_q0, window_size)\n",
    "            # Forecast Q0\n",
    "            preds0_scaled = model.predict(X0, verbose=0).flatten()\n",
    "            preds0 = prev_scaler.inverse_transform_close(preds0_scaled)\n",
    "            actual0 = prev_scaler.inverse_transform_close(y0)\n",
    "            current_prices0 = prev_scaler.inverse_transform_close(current0)\n",
    "            timestamps0 = df_q0['open_time'].iloc[window_size:].to_list()\n",
    "            res0 = pd.DataFrame({\n",
    "                'date': timestamps0,\n",
    "                'actual_price': actual0,\n",
    "                'forecasted_price': preds0,\n",
    "                'current_price': current_prices0\n",
    "            })\n",
    "            all_results.append(res0)\n",
    "            \n",
    "            # Train on Q0 before forecasting Q1\n",
    "            model.compile(\n",
    "                loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "            )\n",
    "            model.fit(\n",
    "                X0, y0,\n",
    "                epochs=train_epochs_q0,\n",
    "                batch_size=32,\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "        # 3. Loop through Q1‚ÄìQ4 of 2024\n",
    "        for q in quarters:\n",
    "            q_start = q.start_time\n",
    "            q_end = q.end_time\n",
    "            q_df = df[(df['open_time'] >= q_start) & (df['open_time'] <= q_end)].copy()\n",
    "            if q_df.empty:\n",
    "                continue\n",
    "\n",
    "            # Prepare and scale data\n",
    "            X, y_true, current_scaled, prev_scaler = prepare_quarter_data(\n",
    "                q_df, window_size, scaler=prev_scaler\n",
    "            )\n",
    "            # Forecast\n",
    "            preds_scaled = model.predict(X, verbose=0).flatten()\n",
    "            preds = prev_scaler.inverse_transform_close(preds_scaled)\n",
    "            actuals = prev_scaler.inverse_transform_close(y_true)\n",
    "            currents = prev_scaler.inverse_transform_close(current_scaled)\n",
    "            timestamps = q_df['open_time'].iloc[window_size:].to_list()\n",
    "\n",
    "            res_df = pd.DataFrame({\n",
    "                'date': timestamps,\n",
    "                'actual_price': actuals,\n",
    "                'forecasted_price': preds,\n",
    "                'current_price': currents\n",
    "            })\n",
    "            all_results.append(res_df)\n",
    "\n",
    "            # Train on this quarter before next\n",
    "            model.compile(\n",
    "                loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "            )\n",
    "            model.fit(\n",
    "                X, y_true,\n",
    "                epochs=train_epochs,\n",
    "                batch_size=32,\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "        # 4. Save all results (Q0 + Q1-Q4) to a single file\n",
    "        if all_results:\n",
    "            out = pd.concat(all_results, ignore_index=True)\n",
    "            os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "            out.to_csv(\n",
    "                os.path.join(OUTPUT_DIR, f\"{asset}usdt_forecasted.csv\"),\n",
    "                index=False\n",
    "            )\n",
    "\n",
    "ASSETS = ['btc', 'eth', 'sol', 'avax', 'bnb', 'pca']      # whatever tickers you care about\n",
    "forecast_quarterly(\n",
    "    ASSETS,\n",
    "    DATA_DIR='data',\n",
    "    MODEL_DIR='models',\n",
    "    OUTPUT_DIR='Forecast'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12e2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization (best weight allocation plus Sharpe ratio and DAO weight correlation per hour)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"üöÄ April optimization script started...\")\n",
    "\n",
    "# Settings\n",
    "ASSETS = ['btc', 'eth', 'sol', 'avax', 'bnb']\n",
    "PCA_ASSET = 'pca'\n",
    "RISK_FREE_RATE = 0.044 / (365 * 24)\n",
    "TOTAL_SAMPLES = 500_000\n",
    "ALPHAS = [0.5, 1.0, 2.0]\n",
    "WINDOW_HOURS = 168\n",
    "CHECKPOINT_PATH = \"april_optimization_checkpoint.pkl\"\n",
    "PCA_ANALYSIS_POINTS = 100\n",
    "TEST_MODE = False\n",
    "\n",
    "# Date filter for April 2024\n",
    "def in_april(dt):\n",
    "    return datetime(2023, 12, 16) <= dt < datetime(2024, 12, 31)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dirichlet sampling\n",
    "def sample_dirichlet_weights(num_samples, num_assets, alpha=1.0, pca_idx=None, pca_weight=None):\n",
    "    if pca_idx is not None and pca_weight is not None:\n",
    "        non_pca = np.random.dirichlet([alpha] * (num_assets-1), num_samples)\n",
    "        W = np.zeros((num_samples, num_assets))\n",
    "        W[:, pca_idx] = pca_weight\n",
    "        W[:, np.arange(num_assets) != pca_idx] = (1 - pca_weight) * non_pca\n",
    "        return W\n",
    "    return np.random.dirichlet([alpha] * num_assets, num_samples)\n",
    "\n",
    "# Optimization function\n",
    "def optimize_weights_torch(mean_vec, cov_matrix, asset_names, rfr, device,\n",
    "                            num_samples, alphas, pca_analysis=False, pca_idx=None):\n",
    "    best = {'Sharpe': -np.inf}\n",
    "    n = len(asset_names)\n",
    "    if pca_analysis:\n",
    "        res = []\n",
    "        pca_ws = np.linspace(0, 1, PCA_ANALYSIS_POINTS)\n",
    "        for w in pca_ws:\n",
    "            W = sample_dirichlet_weights(num_samples // PCA_ANALYSIS_POINTS, n, 1.0, pca_idx, w)\n",
    "            T = torch.tensor(W, dtype=torch.float32, device=device)\n",
    "            ret = T @ mean_vec\n",
    "            W1 = T.unsqueeze(1)\n",
    "            C = cov_matrix.expand(T.size(0), n, n)\n",
    "            var = torch.bmm(W1, torch.bmm(C, W1.transpose(1,2))).squeeze()\n",
    "            std = torch.sqrt(var)\n",
    "            sharpe = (ret - rfr) / (std + 1e-8)\n",
    "            idx_max = torch.argmax(sharpe)\n",
    "            res.append({\n",
    "                'PCA Weight': w,\n",
    "                'Sharpe': sharpe[idx_max].item()\n",
    "            })\n",
    "        return None, pd.DataFrame(res)\n",
    "    else:\n",
    "        for a in alphas:\n",
    "            W = sample_dirichlet_weights(num_samples // len(alphas), n, a)\n",
    "            T = torch.tensor(W, dtype=torch.float32, device=device)\n",
    "            ret = T @ mean_vec\n",
    "            W1 = T.unsqueeze(1)\n",
    "            C = cov_matrix.expand(T.size(0), n, n)\n",
    "            var = torch.bmm(W1, torch.bmm(C, W1.transpose(1,2))).squeeze()\n",
    "            std = torch.sqrt(var)\n",
    "            sharpe = (ret - rfr) / (std + 1e-8)\n",
    "            sharpe[torch.isnan(sharpe)] = -float('inf')\n",
    "            sharpe[torch.isinf(sharpe)] = -float('inf')\n",
    "            im = torch.argmax(sharpe)\n",
    "            if sharpe[im] > best['Sharpe']:\n",
    "                best = {**{f'{asset} Weight': T[im][i].item() for i, asset in enumerate(asset_names)},\n",
    "                        'Sharpe': sharpe[im].item()}\n",
    "        return best, None\n",
    "\n",
    "# Load data\n",
    "returns_f, returns_a, prices = {}, {}, {}\n",
    "for asset in ASSETS + [PCA_ASSET]:\n",
    "    path = f\"Forecast/{asset}usdt_forecasted.csv\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"‚ö†Ô∏è Missing {asset}, skipping\")\n",
    "        continue\n",
    "    df = pd.read_csv(path, parse_dates=['date']).dropna()\n",
    "    df['forecasted_return'] = (df['forecasted_price'] - df['current_price']) / df['current_price']\n",
    "    df['actual_return']   = (df['actual_price']   - df['current_price']) / df['current_price']\n",
    "    returns_f[asset] = df.set_index('date')['forecasted_return']\n",
    "    returns_a[asset] = df.set_index('date')['actual_return']\n",
    "    prices[asset]     = df.set_index('date')['actual_price']\n",
    "\n",
    "# Align dates and filter April\n",
    "f_df = pd.DataFrame(returns_f).dropna()\n",
    "a_df = pd.DataFrame(returns_a).dropna()\n",
    "e_df = pd.DataFrame(prices).pct_change().shift(1).dropna()\n",
    "dates = sorted(set(f_df.index) & set(a_df.index) & set(e_df.index))\n",
    "apr_dates = [d for d in dates if in_april(d)]\n",
    "\n",
    "# Prepare results\n",
    "corr_records = []\n",
    "best_forecasted = {'Sharpe': -np.inf}\n",
    "best_expected   = {'Sharpe': -np.inf}\n",
    "n = len(ASSETS) + 1\n",
    "pca_i = list(f_df.columns).index(PCA_ASSET)\n",
    "\n",
    "# Main loop over April hours\n",
    "for i, date in enumerate(apr_dates):\n",
    "    print(f\"üïí [{i+1}/{len(apr_dates)}] Optimization started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} for hour: {date}\")\n",
    "    start = date - timedelta(hours=WINDOW_HOURS)\n",
    "    f_win = f_df.loc[start:date]\n",
    "    a_win = a_df.loc[start:date]\n",
    "    e_win = e_df.loc[start:date]\n",
    "    if len(f_win) < WINDOW_HOURS or len(e_win) < WINDOW_HOURS:\n",
    "        continue\n",
    "    f_mu = torch.tensor(f_win.mean().values, dtype=torch.float32, device=device)\n",
    "    f_cov= torch.tensor(f_win.cov().values, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    a_mu = torch.tensor(e_win.mean().values, dtype=torch.float32, device=device)\n",
    "    a_cov= torch.tensor(a_win.cov().values, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    # Correlation analysis for this hour\n",
    "    _, f_pca = optimize_weights_torch(f_mu, f_cov, list(f_win.columns), RISK_FREE_RATE, device, TOTAL_SAMPLES, ALPHAS, True, pca_i)\n",
    "    _, a_pca = optimize_weights_torch(a_mu, a_cov, list(a_win.columns), RISK_FREE_RATE, device, TOTAL_SAMPLES, ALPHAS, True, pca_i)\n",
    "    corr_records.append({\n",
    "        'date': date,\n",
    "        'forecasted_corr': f_pca['PCA Weight'].corr(f_pca['Sharpe']),\n",
    "        'expected_corr':   a_pca['PCA Weight'].corr(a_pca['Sharpe'])\n",
    "    })\n",
    "    print(f\"‚úÖ Correlation analysis done for {date} ‚Äî Forecasted corr: {corr_records[-1]['forecasted_corr']:.4f}, Expected corr: {corr_records[-1]['expected_corr']:.4f}\")\n",
    "    # Track best overall\n",
    "    f_best, _ = optimize_weights_torch(f_mu, f_cov, list(f_win.columns), RISK_FREE_RATE, device, TOTAL_SAMPLES, ALPHAS)\n",
    "    a_best, _ = optimize_weights_torch(a_mu, a_cov, list(a_win.columns), RISK_FREE_RATE, device, TOTAL_SAMPLES, ALPHAS)\n",
    "    if f_best['Sharpe'] > best_forecasted['Sharpe']: best_forecasted = {**f_best, 'date': date}\n",
    "    if a_best['Sharpe'] > best_expected['Sharpe']:   best_expected   = {**a_best, 'date': date}\n",
    "\n",
    "# Save correlation CSV\n",
    "pd.DataFrame(corr_records).to_csv('expected_weightVSsharpe_correlation.csv', index=False)\n",
    "print(\"üìÇ Saved per-hour correlations.\")\n",
    "\n",
    "# Save best allocations\n",
    "best_f_df = pd.DataFrame([best_forecasted])\n",
    "best_a_df = pd.DataFrame([best_expected])\n",
    "best_f_df.to_csv('best_year_forecasted_portfolios.csv', index=False)\n",
    "best_a_df.to_csv('best_year_expected_portfolios.csv', index=False)\n",
    "print(\"üìÇ Saved best April portfolios.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b767fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAO weight Allocation and Sharpe Ratio progression plotting\n",
    "# Event divided plot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# =============================================\n",
    "# 1. Data Loading and Preparation\n",
    "# =============================================\n",
    "\n",
    "# Load portfolio weights and Sharpe data\n",
    "df_weights = pd.read_csv('best_pca_forecasted_portfolios.csv', parse_dates=['date'])\n",
    "layer1_assets = ['btc', 'eth', 'sol', 'avax', 'bnb']\n",
    "dao_assets   = ['pca']\n",
    "\n",
    "# Load price data for assets\n",
    "assets = layer1_assets + dao_assets\n",
    "price_data = {}\n",
    "for asset in assets:\n",
    "    file_path = os.path.join('Forecast', f'{asset}usdt_forecasted.csv')\n",
    "    try:\n",
    "        df_price = pd.read_csv(file_path, parse_dates=['date'])\n",
    "        price_data[asset] = df_price.set_index('date')['actual_price']\n",
    "    except (FileNotFoundError, KeyError):\n",
    "        # fallback to 'close'\n",
    "        try:\n",
    "            price_data[asset] = df_price.set_index('date')['close']\n",
    "        except:\n",
    "            print(f\"Skipping {asset}: no price column found.\")\n",
    "\n",
    "prices  = pd.concat(price_data, axis=1)\n",
    "returns = prices.pct_change().dropna()\n",
    "\n",
    "# =============================================\n",
    "# 2. Weights & Returns Alignment\n",
    "# =============================================\n",
    "df_weights['Layer1_Weight'] = df_weights[[f'{a} Weight' for a in layer1_assets]].sum(axis=1)\n",
    "df_weights['DAO_Weight']    = df_weights[[f'{a} Weight' for a in dao_assets]].sum(axis=1)\n",
    "\n",
    "# Daily mean weights\n",
    "df_daily = (df_weights\n",
    "    .assign(day=lambda d: d['date'].dt.date)\n",
    "    .groupby('day')[['Layer1_Weight','DAO_Weight','Sharpe']]\n",
    "    .mean()\n",
    "    .rename_axis('day')\n",
    "    .reset_index()\n",
    "    .assign(date=lambda d: pd.to_datetime(d['day']))\n",
    ")\n",
    "\n",
    "df_combined = df_daily.merge(returns, on='date', how='left')\n",
    "df_combined['Layer1_Weight_Smoothed'] = df_combined['Layer1_Weight'].rolling(3, min_periods=1).mean()\n",
    "df_combined['DAO_Weight_Smoothed']    = df_combined['DAO_Weight'].rolling(3, min_periods=1).mean()\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# 3. Combined Visualization with Event Shading\n",
    "# =============================================\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, figsize=(20,18), sharex=True)\n",
    "plt.suptitle('DAO Asset Analysis Dashboard', y=1.02, fontsize=18)\n",
    "\n",
    "# -- Top: DAO Weight --\n",
    "ax1.plot(df_combined['date'], df_combined['DAO_Weight_Smoothed'],\n",
    "         label='DAO Weight', linewidth=2, color='#ff7f0e')\n",
    "ax1.set_ylabel('DAO Weight')\n",
    "ax1.set_title('DAO Weight Over Time')\n",
    "\n",
    "# -- Mid: Sharpe Ratio --\n",
    "ax2.plot(df_combined['date'], df_combined['Sharpe'],\n",
    "         label='Sharpe Ratio', linewidth=2, color='purple')\n",
    "ax2.set_ylabel('Sharpe Ratio')\n",
    "ax2.set_title('Portfolio Sharpe Ratio')\n",
    "\n",
    "# -- X‚Äëaxis formatting --\n",
    "for ax in (ax1, ax2):\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "    ax.xaxis.set_minor_locator(mdates.WeekdayLocator(byweekday=mdates.MO))\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e15536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAO weight allocation and Sharpe Ratio correlation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_normal_scatter(csv_file_path: str = 'pca_expected_effect.csv'):\n",
    "    \"\"\"\n",
    "    Creates a normal scatter plot with:\n",
    "    - X-axis: Sharpe Ratio\n",
    "    - Y-axis: PCA Weight\n",
    "    (Raw data points only, no enforced centering)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read and prepare data\n",
    "    try:\n",
    "        data = pd.read_csv(csv_file_path)\n",
    "        sharpe = data['Sharpe'].values\n",
    "        pca_weight = data['PCA Weight'].values\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Create plot with professional styling\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot raw data points with improved styling\n",
    "    plt.scatter(sharpe, pca_weight, \n",
    "                color='steelblue', \n",
    "                s=80,\n",
    "                alpha=0.8,\n",
    "                edgecolors='white',\n",
    "                linewidth=1,\n",
    "                label='Portfolio Allocations')\n",
    "    \n",
    "    # Add light zero lines (optional)\n",
    "    plt.axhline(0, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "    plt.axvline(0, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "    \n",
    "    # Automatic axis limits based on data\n",
    "    x_padding = (sharpe.max() - sharpe.min()) * 0.1\n",
    "    y_padding = (pca_weight.max() - pca_weight.min()) * 0.1\n",
    "    \n",
    "    plt.xlim(sharpe.min() - x_padding, sharpe.max() + x_padding)\n",
    "    plt.ylim(pca_weight.min() - y_padding, pca_weight.max() + y_padding)\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel('Sharpe Ratio', fontsize=12)\n",
    "    plt.ylabel('PCA Weight', fontsize=12)\n",
    "    plt.title('PCA Weight vs Sharpe Ratio Distribution (Expected Optimization)', fontsize=14, pad=15)\n",
    "    \n",
    "    # Grid and aesthetics\n",
    "    plt.grid(True, linestyle=':', alpha=0.3)\n",
    "    \n",
    "    # Remove top/right spines and adjust left/bottom\n",
    "    for spine in ['top', 'right']:\n",
    "        plt.gca().spines[spine].set_visible(False)\n",
    "    for spine in ['left', 'bottom']:\n",
    "        plt.gca().spines[spine].set_linewidth(0.5)\n",
    "        plt.gca().spines[spine].set_color('gray')\n",
    "    \n",
    "    # Add data density indicators if many points\n",
    "    if len(sharpe) > 50:\n",
    "        from scipy.stats import gaussian_kde\n",
    "        try:\n",
    "            xy = np.vstack([sharpe, pca_weight])\n",
    "            z = gaussian_kde(xy)(xy)\n",
    "            plt.scatter(sharpe, pca_weight, c=z, s=40, cmap='viridis', alpha=0.6)\n",
    "            plt.colorbar(label='Data Density')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the function\n",
    "plot_normal_scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460360d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAO weight allocation and Standard Deviation correlation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def plot_std_scatter(csv_file_path: str = 'pca_forecasted_effect.csv'):\n",
    "    \"\"\"\n",
    "    Creates a scatter plot with:\n",
    "    - X-axis: PCA Weight\n",
    "    - Y-axis: Standard Deviation\n",
    "    - Color-coded data density\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read and prepare data\n",
    "    try:\n",
    "        data = pd.read_csv(csv_file_path)\n",
    "        std_dev = data['Standard Deviation'].values  # Standard Deviation values\n",
    "        pca_weight = data['PCA Weight'].values\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Create plot with professional styling\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Calculate point density for coloring (using swapped axes)\n",
    "    xy = np.vstack([pca_weight, std_dev])  # Swapped order\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    \n",
    "    # Sort points by density (so densest points appear on top)\n",
    "    idx = z.argsort()\n",
    "    pca_weight, std_dev, z = pca_weight[idx], std_dev[idx], z[idx]\n",
    "    \n",
    "    # Plot data points with density-based coloring (axes swapped)\n",
    "    scatter = plt.scatter(pca_weight, std_dev, \n",
    "                          c=z, \n",
    "                          s=80,\n",
    "                          alpha=0.7,\n",
    "                          cmap='viridis',\n",
    "                          edgecolors='w',\n",
    "                          linewidth=0.8)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('Data Density', fontsize=12)\n",
    "    \n",
    "    # Add zero lines\n",
    "    plt.axhline(0, color='gray', linestyle='--', linewidth=0.8, alpha=0.7)\n",
    "    plt.axvline(0, color='gray', linestyle='--', linewidth=0.8, alpha=0.7)\n",
    "    \n",
    "    # Set axis limits (using swapped axes)\n",
    "    x_padding = (pca_weight.max() - pca_weight.min()) * 0.1\n",
    "    y_padding = (std_dev.max() - std_dev.min()) * 0.1\n",
    "    plt.xlim(pca_weight.min() - x_padding, pca_weight.max() + x_padding)\n",
    "    plt.ylim(std_dev.min() - y_padding, std_dev.max() + y_padding)\n",
    "    \n",
    "    # Labels and title (updated for swapped axes)\n",
    "    plt.xlabel('PCA Weight', fontsize=12)\n",
    "    plt.ylabel('Standard Deviation', fontsize=12)\n",
    "    plt.title('Portfolio Standard Deviation vs PCA Weight', fontsize=14, pad=15)\n",
    "    \n",
    "    # Grid and aesthetics\n",
    "    plt.grid(True, linestyle=':', alpha=0.3)\n",
    "    \n",
    "    # Remove top/right spines and adjust left/bottom\n",
    "    for spine in ['top', 'right']:\n",
    "        plt.gca().spines[spine].set_visible(False)\n",
    "    for spine in ['left', 'bottom']:\n",
    "        plt.gca().spines[spine].set_linewidth(0.5)\n",
    "        plt.gca().spines[spine].set_color('gray')\n",
    "    \n",
    "    # Adjust colorbar ticks\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the function\n",
    "plot_std_scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market regime base - Three Theory Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# === 1. Load data ===\n",
    "opt_df = pd.read_csv(\"expected_weightVSsharpe_correlation.csv\",\n",
    "                     parse_dates=[\"date\"], index_col=\"date\")\n",
    "dao = pd.read_csv(\"data/pcausdt_hourly_indicators.csv\",\n",
    "                  parse_dates=[\"open_time\"], index_col=\"open_time\")\n",
    "l1 = pd.read_csv(\"data/lyrusdt_hourly_indicators.csv\",\n",
    "                 parse_dates=[\"open_time\"], index_col=\"open_time\")\n",
    "\n",
    "common = opt_df.index.intersection(dao.index).intersection(l1.index)\n",
    "opt_df, dao, l1 = opt_df.loc[common], dao.loc[common], l1.loc[common]\n",
    "\n",
    "# === 2. Compute hourly indicators ===\n",
    "dao_ret = dao[\"close\"].pct_change()\n",
    "l1_ret = l1[\"close\"].pct_change()\n",
    "drawdown = (l1[\"close\"] - l1[\"close\"].cummax()) / l1[\"close\"].cummax()\n",
    "mark_corr = dao_ret.rolling(24).corr(l1_ret)\n",
    "\n",
    "betas, idx = [], []\n",
    "for t in range(24, len(dao_ret)):\n",
    "    wdr = dao_ret.iloc[t-24:t]\n",
    "    wlr = l1_ret.iloc[t-24:t]\n",
    "    if wdr.isnull().any() or wlr.isnull().any():\n",
    "        betas.append(np.nan)\n",
    "    else:\n",
    "        X = sm.add_constant(wlr)\n",
    "        betas.append(sm.OLS(wdr, X).fit().params[1])\n",
    "    idx.append(dao_ret.index[t])\n",
    "beta_series = pd.Series(betas, index=idx)\n",
    "\n",
    "illiquidity = (dao_ret.abs() / dao[\"volume\"]).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "df_hr = pd.DataFrame({\n",
    "    \"opt_corr\":    opt_df[\"expected_corr\"],\n",
    "    \"drawdown\":    drawdown,\n",
    "    \"mark_corr\":   mark_corr,\n",
    "    \"beta\":        beta_series,\n",
    "    \"illiquidity\": illiquidity\n",
    "}).dropna()\n",
    "\n",
    "for col in [\"drawdown\", \"mark_corr\", \"beta\", \"illiquidity\"]:\n",
    "    df_hr[f\"{col}_lag\"] = df_hr[col].shift(1)\n",
    "df_hr = df_hr.dropna()\n",
    "\n",
    "# === 3. Determine regime via moving average crossover ===\n",
    "price = l1[\"close\"].resample(\"D\").last()\n",
    "fast = price.rolling(window=50, min_periods=1).mean()\n",
    "slow = price.rolling(window=200, min_periods=1).mean()\n",
    "\n",
    "daily_regimes = pd.Series(index=price.index, dtype=\"object\")\n",
    "daily_regimes[fast > slow] = \"bull\"\n",
    "daily_regimes[fast < slow] = \"bear\"\n",
    "daily_regimes[(fast == slow)] = \"neutral\"\n",
    "\n",
    "monthly_ma_regimes = (daily_regimes\n",
    "                      .to_frame(\"regime\")\n",
    "                      .assign(month=lambda df: df.index.to_period(\"M\"))\n",
    "                      .groupby(\"month\")[\"regime\"]\n",
    "                      .agg(lambda x: x.mode()[0]))\n",
    "\n",
    "# === 4. Correlation by month ===\n",
    "df_hr[\"month\"] = df_hr.index.to_period(\"M\")\n",
    "df_hr[\"regime\"] = df_hr[\"month\"].map(monthly_ma_regimes)\n",
    "\n",
    "records = []\n",
    "\n",
    "for mon, grp in df_hr.groupby(\"month\"):\n",
    "    regime = grp[\"regime\"].iloc[0]\n",
    "    corr_draw  = grp[[\"opt_corr\",\"drawdown_lag\"]].corr().iloc[0,1]\n",
    "    corr_mark  = grp[[\"opt_corr\",\"mark_corr_lag\"]].corr().iloc[0,1]\n",
    "    corr_beta  = grp[[\"opt_corr\",\"beta_lag\"]].corr().iloc[0,1]\n",
    "    corr_illiq = grp[[\"opt_corr\",\"illiquidity_lag\"]].corr().iloc[0,1]\n",
    "    \n",
    "    records.append({\n",
    "        \"month\": mon.strftime(\"%Y-%m\"),\n",
    "        \"regime\": regime,\n",
    "        \"draw\": corr_draw,\n",
    "        \"mark\": corr_mark,\n",
    "        \"beta\": corr_beta,\n",
    "        \"ill\":  corr_illiq\n",
    "    })\n",
    "\n",
    "# === 5. Summarize correlation by regime ===\n",
    "df_results = pd.DataFrame(records)\n",
    "summary = df_results.groupby(\"regime\")[[\"draw\", \"mark\", \"beta\", \"ill\"]].mean().round(3)\n",
    "\n",
    "print(\"=== Average Lagged Correlations by Regime ===\")\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
